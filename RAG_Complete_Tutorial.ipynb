{"cells":[{"cell_type":"markdown","metadata":{"id":"5liGgWP4PU7O"},"source":["#  Complete Guide to Retrieval-Augmented Generation (RAG)\n","## From Zero to Production: A Comprehensive Tutorial\n","\n","**Author:** Jaswanth Gurujala\n","**Course:** INFO 7390 - Advanced Data Science and Architecture  \n","\n","\n","---\n","\n","##  What You'll Learn\n","\n","By the end of this notebook, you will be able to:\n","1.  Understand what RAG is and why it matters\n","2.  Build a simple RAG system from scratch\n","3.  Implement advanced RAG techniques (chunking, hybrid search, re-ranking)\n","4.  Evaluate and optimize RAG performance\n","5.  Deploy a production-ready RAG application\n","\n","---\n","\n","##  Learning Path\n","\n","```\n","Part 1: Foundations â†’ Part 2: Simple RAG â†’ Part 3: Advanced RAG â†’ Part 4: Evaluation â†’ Part 5: Production\n","```"]},{"cell_type":"markdown","metadata":{"id":"mA3YmVJhPU7P"},"source":["---\n","# Part 1: Understanding RAG - The \"Why\" and \"What\"\n","\n","##  The Problem RAG Solves\n","\n","### Scenario: Imagine you're building a chatbot for your company\n","\n","**Without RAG:**\n","- LLM only knows information from its training data (cutoff date)\n","- Can't access your company's internal documents\n","- Might hallucinate answers about your specific context\n","- Can't update without retraining (expensive!)\n","\n","**With RAG:**\n","- LLM retrieves relevant information from your documents\n","- Grounds responses in actual company data\n","- Updates instantly when you add new documents\n","- Much cheaper than fine-tuning\n","\n","---\n","\n","##  RAG Architecture: The Big Picture\n","\n","```\n","â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n","â”‚                     RAG SYSTEM                              â”‚\n","â”‚                                                             â”‚\n","â”‚  1. INDEXING (Offline)          2. RETRIEVAL (Runtime)     â”‚\n","â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n","â”‚  â”‚  Documents      â”‚             â”‚ User Query   â”‚          â”‚\n","â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n","â”‚           â”‚                              â”‚                  â”‚\n","â”‚           â–¼                              â–¼                  â”‚\n","â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n","â”‚  â”‚ Text Splitting  â”‚             â”‚  Embedding   â”‚          â”‚\n","â”‚  â”‚  (Chunking)     â”‚             â”‚    Model     â”‚          â”‚\n","â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n","â”‚           â”‚                              â”‚                  â”‚\n","â”‚           â–¼                              â–¼                  â”‚\n","â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n","â”‚  â”‚  Embedding      â”‚             â”‚   Search     â”‚          â”‚\n","â”‚  â”‚    Model        â”‚             â”‚  Vector DB   â”‚          â”‚\n","â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n","â”‚           â”‚                              â”‚                  â”‚\n","â”‚           â–¼                              â–¼                  â”‚\n","â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n","â”‚  â”‚  Vector DB      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  Top-K Docs  â”‚          â”‚\n","â”‚  â”‚   (Storage)     â”‚             â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n","â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚                  â”‚\n","â”‚                                          â–¼                  â”‚\n","â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n","â”‚                              â”‚  LLM Generation    â”‚         â”‚\n","â”‚                              â”‚ (Query + Context)  â”‚         â”‚\n","â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n","â”‚                                        â”‚                    â”‚\n","â”‚                                        â–¼                    â”‚\n","â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n","â”‚                              â”‚   Final Answer     â”‚         â”‚\n","â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","```\n","\n","---\n","\n","##  Key Concepts\n","\n","### 1. **Embeddings**\n","Converting text into numerical vectors that capture semantic meaning\n","- Similar texts â†’ Similar vectors\n","- Example: \"dog\" and \"puppy\" are close in vector space\n","\n","### 2. **Vector Database**\n","Specialized database for storing and searching embeddings efficiently\n","- Examples: Pinecone, Chroma, FAISS, Weaviate\n","\n","### 3. **Retrieval**\n","Finding the most relevant documents for a query\n","- Similarity search (cosine, dot product)\n","- Returns Top-K most relevant chunks\n","\n","### 4. **Generation**\n","LLM uses retrieved context to generate an answer\n","- Context injection into prompt\n","- Grounded, fact-based responses\n","\n","---\n","\n","##  When to Use RAG vs. Alternatives\n","\n","| Approach | Best For | Cost | Update Speed |\n","|----------|----------|------|-------------|\n","| **RAG** | External knowledge, frequent updates |  Low |  Instant |\n","| **Fine-tuning** | Style/tone adaptation, domain-specific language |  High |  Slow (retrain) |\n","| **Prompt Engineering** | Task-specific instructions |  Very Low |  Instant |\n","| **Pre-training** | General intelligence |  Extreme |  Very Slow |\n","\n","**Use RAG when you need:**\n","- Up-to-date information\n","- Company/domain-specific knowledge\n","- Traceable, citeable sources\n","- Cost-effective solution"]},{"cell_type":"markdown","metadata":{"id":"1LQQh_cSPU7Q"},"source":["---\n","# Part 2: Building Your First RAG System\n","\n","Let's build a simple but functional RAG system step-by-step!"]},{"cell_type":"markdown","metadata":{"id":"Y4q6yPHSPU7Q"},"source":["##  Setup & Installation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMxCchs3PU7Q"},"outputs":[],"source":["# Install required packages\n","!pip install -q openai sentence-transformers chromadb langchain tiktoken numpy pandas matplotlib seaborn plotly python-dotenv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RTUi1aqoPU7R"},"outputs":[],"source":["# Import libraries\n","import os\n","import numpy as np\n","import pandas as pd\n","from typing import List, Dict, Tuple\n","import re\n","from collections import defaultdict\n","\n","# Embeddings and Vector Store\n","from sentence_transformers import SentenceTransformer\n","import chromadb\n","from chromadb.config import Settings\n","\n","# LLM Integration\n","from openai import OpenAI\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.graph_objects as go\n","import plotly.express as px\n","\n","# Set style\n","sns.set_style('whitegrid')\n","plt.rcParams['figure.figsize'] = (12, 6)\n","\n","# Warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\" All libraries imported successfully!\")"]},{"cell_type":"markdown","metadata":{"id":"6PyPNgQXPU7R"},"source":["##  API Configuration\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fNLiHzH4PU7R"},"outputs":[],"source":["# Option 1: Set API key directly (not recommended for production)\n","# os.environ['OPENAI_API_KEY'] = 'your-api-key-here'\n","\n","# Option 2: Load from .env file (recommended)\n","from dotenv import load_dotenv\n","load_dotenv()\n","\n","# Initialize OpenAI client\n","client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n","\n","print(\" API client initialized!\")"]},{"cell_type":"markdown","metadata":{"id":"0h6cTqGXPU7R"},"source":["---\n","##  Step 1: Create Sample Documents\n","\n","Let's create a knowledge base about data science concepts from our course!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7dYSlWlQPU7R"},"outputs":[],"source":["# Sample documents about data science concepts\n","documents = [\n","    {\n","        \"id\": \"doc1\",\n","        \"title\": \"GIGO Principle\",\n","        \"content\": \"\"\"The GIGO (Garbage In, Garbage Out) principle is fundamental to data science.\n","        It states that the quality of output is determined by the quality of input data.\n","        No matter how sophisticated your model is, poor quality data will lead to poor results.\n","        Data scientists must invest significant time in data validation, cleaning, and quality assessment\n","        before building models. This includes checking for missing values, outliers, inconsistencies,\n","        and ensuring data accurately represents the problem domain.\"\"\"\n","    },\n","    {\n","        \"id\": \"doc2\",\n","        \"title\": \"Causal Inference\",\n","        \"content\": \"\"\"Causal inference is the process of determining the independent, actual effect of a\n","        particular phenomenon that is a component of a larger system. Unlike correlation, which simply\n","        measures association, causation implies that one variable directly influences another.\n","        Common methods include randomized controlled trials (RCTs), propensity score matching,\n","        instrumental variables, and difference-in-differences analysis. These methods help us answer\n","        'what if' questions and understand the true impact of interventions.\"\"\"\n","    },\n","    {\n","        \"id\": \"doc3\",\n","        \"title\": \"Time Series Forecasting\",\n","        \"content\": \"\"\"Time series forecasting involves predicting future values based on historical data points\n","        collected over time. Key components include trend (long-term direction), seasonality (regular patterns),\n","        and noise (random fluctuations). Popular methods include ARIMA (AutoRegressive Integrated Moving Average),\n","        Prophet (developed by Facebook), and LSTM networks (Long Short-Term Memory neural networks).\n","        Applications range from stock price prediction to weather forecasting and demand planning.\"\"\"\n","    },\n","    {\n","        \"id\": \"doc4\",\n","        \"title\": \"A/B Testing\",\n","        \"content\": \"\"\"A/B testing is a randomized experiment comparing two or more variants to determine\n","        which performs better. It's crucial for making data-driven decisions in product development.\n","        Key considerations include sample size calculation, statistical significance, p-values,\n","        and avoiding peeking at results too early. Common pitfalls include selection bias,\n","        insufficient sample size, and not accounting for multiple testing. Modern approaches include\n","        multi-armed bandits and sequential testing for faster iteration.\"\"\"\n","    },\n","    {\n","        \"id\": \"doc5\",\n","        \"title\": \"Feature Engineering\",\n","        \"content\": \"\"\"Feature engineering is the process of creating new features from raw data to improve\n","        model performance. It's often more impactful than algorithm selection. Techniques include:\n","        polynomial features (creating interactions), encoding categorical variables (one-hot, target encoding),\n","        handling missing values (imputation strategies), scaling (normalization, standardization),\n","        and domain-specific transformations. Good feature engineering requires deep understanding\n","        of both the data and the problem domain. It's an iterative process that combines domain expertise\n","        with statistical techniques.\"\"\"\n","    },\n","    {\n","        \"id\": \"doc6\",\n","        \"title\": \"Model Evaluation Metrics\",\n","        \"content\": \"\"\"Choosing the right evaluation metric is crucial for model success. For classification:\n","        accuracy (overall correctness), precision (correctness of positive predictions), recall (coverage\n","        of actual positives), F1-score (harmonic mean of precision and recall), and ROC-AUC (discrimination ability).\n","        For regression: MSE (mean squared error), RMSE (root mean squared error), MAE (mean absolute error),\n","        and R-squared (proportion of variance explained). The choice depends on business context - for example,\n","        in fraud detection, recall might be more important than precision.\"\"\"\n","    },\n","    {\n","        \"id\": \"doc7\",\n","        \"title\": \"Deep Learning Fundamentals\",\n","        \"content\": \"\"\"Deep learning uses neural networks with multiple layers to learn hierarchical representations\n","        of data. Key architectures include: CNNs (Convolutional Neural Networks) for image processing,\n","        RNNs (Recurrent Neural Networks) for sequential data, Transformers for natural language processing,\n","        and GANs (Generative Adversarial Networks) for generation tasks. Training involves backpropagation,\n","        gradient descent optimization, and regularization techniques like dropout and batch normalization.\n","        Deep learning excels with large datasets but requires significant computational resources.\"\"\"\n","    },\n","    {\n","        \"id\": \"doc8\",\n","        \"title\": \"Bias and Fairness in ML\",\n","        \"content\": \"\"\"Machine learning models can perpetuate and amplify societal biases present in training data.\n","        Types of bias include historical bias (biased data collection), representation bias (unrepresentative samples),\n","        and measurement bias (flawed metrics). Fairness metrics include demographic parity (equal positive rates\n","        across groups), equalized odds (equal true/false positive rates), and individual fairness (similar individuals\n","        treated similarly). Mitigation strategies involve careful data collection, bias-aware algorithms,\n","        and continuous monitoring in production. Ethical AI requires ongoing vigilance and diverse perspectives.\"\"\"\n","    }\n","]\n","\n","print(f\" Created {len(documents)} sample documents\")\n","print(f\"\\nDocument titles:\")\n","for doc in documents:\n","    print(f\"  â€¢ {doc['title']}\")"]},{"cell_type":"markdown","metadata":{"id":"Vc-pXeTuPU7S"},"source":["---\n","##  Step 2: Text Chunking\n","\n","**Why chunk?**\n","- Documents are often too long for LLM context windows\n","- Smaller chunks provide more precise retrieval\n","- Balance: too small = loss of context, too large = less precision\n","\n","**Common strategies:**\n","1. **Fixed-size chunking** - Simple, consistent\n","2. **Sentence-based** - Natural boundaries\n","3. **Paragraph-based** - Semantic units\n","4. **Semantic chunking** - ML-based meaningful splits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ro4irXfmPU7S"},"outputs":[],"source":["class SimpleChunker:\n","    \"\"\"\n","    A simple text chunker that splits documents into overlapping chunks.\n","\n","    Parameters:\n","    -----------\n","    chunk_size : int\n","        Target size of each chunk in characters\n","    overlap : int\n","        Number of overlapping characters between chunks (helps maintain context)\n","    \"\"\"\n","\n","    def __init__(self, chunk_size: int = 500, overlap: int = 50):\n","        self.chunk_size = chunk_size\n","        self.overlap = overlap\n","\n","    def chunk_text(self, text: str, metadata: Dict = None) -> List[Dict]:\n","        \"\"\"\n","        Split text into overlapping chunks.\n","\n","        Returns:\n","        --------\n","        List of dictionaries containing chunk text and metadata\n","        \"\"\"\n","        # Clean the text\n","        text = re.sub(r'\\s+', ' ', text).strip()\n","\n","        chunks = []\n","        start = 0\n","        chunk_id = 0\n","\n","        while start < len(text):\n","            # Define chunk boundaries\n","            end = start + self.chunk_size\n","\n","            # If this isn't the last chunk, try to break at sentence boundary\n","            if end < len(text):\n","                # Look for sentence ending near the chunk boundary\n","                last_period = text.rfind('.', start, end)\n","                if last_period != -1 and last_period > start + self.chunk_size * 0.5:\n","                    end = last_period + 1\n","\n","            # Extract chunk\n","            chunk_text = text[start:end].strip()\n","\n","            # Create chunk object with metadata\n","            chunk = {\n","                'text': chunk_text,\n","                'chunk_id': chunk_id,\n","                'start_pos': start,\n","                'end_pos': end,\n","                'metadata': metadata or {}\n","            }\n","            chunks.append(chunk)\n","\n","            # Move to next chunk with overlap\n","            start = end - self.overlap\n","            chunk_id += 1\n","\n","        return chunks\n","\n","\n","# Initialize chunker\n","chunker = SimpleChunker(chunk_size=400, overlap=50)\n","\n","# Chunk all documents\n","all_chunks = []\n","for doc in documents:\n","    metadata = {\n","        'doc_id': doc['id'],\n","        'title': doc['title']\n","    }\n","    chunks = chunker.chunk_text(doc['content'], metadata)\n","    all_chunks.extend(chunks)\n","\n","print(f\" Created {len(all_chunks)} chunks from {len(documents)} documents\")\n","print(f\"\\nExample chunk:\")\n","print(f\"Title: {all_chunks[0]['metadata']['title']}\")\n","print(f\"Text: {all_chunks[0]['text'][:200]}...\")"]},{"cell_type":"markdown","metadata":{"id":"OvyJWyxZPU7S"},"source":["###  Visualize Chunk Distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4p7i3zWPU7S"},"outputs":[],"source":["# Analyze chunk sizes\n","chunk_lengths = [len(chunk['text']) for chunk in all_chunks]\n","\n","fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","# Histogram\n","axes[0].hist(chunk_lengths, bins=20, color='skyblue', edgecolor='black')\n","axes[0].axvline(np.mean(chunk_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(chunk_lengths):.0f}')\n","axes[0].set_xlabel('Chunk Length (characters)')\n","axes[0].set_ylabel('Frequency')\n","axes[0].set_title('Distribution of Chunk Lengths')\n","axes[0].legend()\n","\n","# Box plot\n","axes[1].boxplot(chunk_lengths, vert=True)\n","axes[1].set_ylabel('Chunk Length (characters)')\n","axes[1].set_title('Chunk Length Statistics')\n","axes[1].grid(axis='y')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(f\"\\n Chunk Statistics:\")\n","print(f\"  Mean length: {np.mean(chunk_lengths):.0f} characters\")\n","print(f\"  Median length: {np.median(chunk_lengths):.0f} characters\")\n","print(f\"  Min length: {min(chunk_lengths)} characters\")\n","print(f\"  Max length: {max(chunk_lengths)} characters\")"]},{"cell_type":"markdown","metadata":{"id":"hXdfA9bJPU7S"},"source":["---\n","##  Step 3: Generate Embeddings\n","\n","**What are embeddings?**\n","- Numerical representations of text that capture semantic meaning\n","- Similar meanings â†’ Similar vectors\n","- Typically 384 to 1536 dimensions\n","\n","**Popular embedding models:**\n","- `all-MiniLM-L6-v2` - Fast, 384 dims, good for most tasks\n","- `all-mpnet-base-v2` - Better quality, 768 dims\n","- OpenAI `text-embedding-ada-002` - 1536 dims, commercial"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YivLweNGPU7S"},"outputs":[],"source":["# Initialize embedding model\n","# Using sentence-transformers (open-source, runs locally)\n","embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","print(f\" Loaded embedding model: all-MiniLM-L6-v2\")\n","print(f\"   Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"galj7bynPU7S"},"outputs":[],"source":["def generate_embeddings(texts: List[str], batch_size: int = 32) -> np.ndarray:\n","    \"\"\"\n","    Generate embeddings for a list of texts.\n","\n","    Parameters:\n","    -----------\n","    texts : List[str]\n","        List of text strings to embed\n","    batch_size : int\n","        Number of texts to process at once (for efficiency)\n","\n","    Returns:\n","    --------\n","    numpy array of shape (n_texts, embedding_dim)\n","    \"\"\"\n","    return embedding_model.encode(texts, batch_size=batch_size, show_progress_bar=True)\n","\n","\n","# Generate embeddings for all chunks\n","chunk_texts = [chunk['text'] for chunk in all_chunks]\n","embeddings = generate_embeddings(chunk_texts)\n","\n","print(f\"\\n Generated embeddings for {len(chunk_texts)} chunks\")\n","print(f\"   Embedding shape: {embeddings.shape}\")\n","print(f\"   Memory usage: {embeddings.nbytes / 1024 / 1024:.2f} MB\")"]},{"cell_type":"markdown","metadata":{"id":"H4aUs2xyPU7T"},"source":["###  Understanding Embeddings: Similarity Example"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8pxnyWcPU7T"},"outputs":[],"source":["# Let's see how embeddings capture semantic similarity\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","test_sentences = [\n","    \"Machine learning is a subset of artificial intelligence\",\n","    \"Deep learning uses neural networks\",\n","    \"I love eating pizza for dinner\",\n","    \"Data science involves statistics and programming\"\n","]\n","\n","test_embeddings = embedding_model.encode(test_sentences)\n","similarity_matrix = cosine_similarity(test_embeddings)\n","\n","# Visualize similarity matrix\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(similarity_matrix, annot=True, fmt='.3f', cmap='YlOrRd',\n","            xticklabels=[f\"S{i+1}\" for i in range(len(test_sentences))],\n","            yticklabels=[f\"S{i+1}\" for i in range(len(test_sentences))],\n","            cbar_kws={'label': 'Cosine Similarity'})\n","plt.title('Semantic Similarity Between Sentences', fontsize=14, fontweight='bold')\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"\\nTest Sentences:\")\n","for i, sent in enumerate(test_sentences, 1):\n","    print(f\"S{i}: {sent}\")\n","\n","print(\"\\nðŸ’¡ Observations:\")\n","print(\"  â€¢ S1 and S2 (ML-related) have high similarity\")\n","print(\"  â€¢ S3 (pizza) has low similarity with tech topics\")\n","print(\"  â€¢ S4 (data science) is similar to S1 and S2\")"]},{"cell_type":"markdown","metadata":{"id":"mxG6gs8hPU7T"},"source":["---\n","##  Step 4: Store in Vector Database\n","\n","**Why use a vector database?**\n","- Efficient similarity search at scale\n","- Optimized storage and indexing\n","- Fast nearest neighbor search\n","\n","We'll use **ChromaDB** - a simple, lightweight vector database perfect for learning!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lvfh0jmuPU7T"},"outputs":[],"source":["# Initialize ChromaDB client\n","chroma_client = chromadb.Client(Settings(\n","    chroma_db_impl=\"duckdb+parquet\",\n","    persist_directory=\"./chroma_db\"  # Where to store the database\n","))\n","\n","# Create or get collection\n","collection_name = \"data_science_docs\"\n","\n","# Delete if exists (for clean slate)\n","try:\n","    chroma_client.delete_collection(collection_name)\n","except:\n","    pass\n","\n","# Create new collection\n","collection = chroma_client.create_collection(\n","    name=collection_name,\n","    metadata={\"description\": \"Data Science course materials\"}\n",")\n","\n","print(f\" Created ChromaDB collection: {collection_name}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IJKQPSsdPU7T"},"outputs":[],"source":["# Add documents to collection\n","collection.add(\n","    documents=[chunk['text'] for chunk in all_chunks],\n","    embeddings=embeddings.tolist(),\n","    metadatas=[chunk['metadata'] for chunk in all_chunks],\n","    ids=[f\"chunk_{i}\" for i in range(len(all_chunks))]\n",")\n","\n","print(f\" Added {len(all_chunks)} chunks to vector database\")\n","print(f\"\\n Collection stats:\")\n","print(f\"   Total documents: {collection.count()}\")\n","print(f\"   Embedding dimension: {len(embeddings[0])}\")"]},{"cell_type":"markdown","metadata":{"id":"iV-x8ONVPU7T"},"source":["---\n","##  Step 5: Implement Retrieval\n","\n","Now let's retrieve relevant documents based on a query!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7XgOOMZNPU7T"},"outputs":[],"source":["def retrieve_documents(query: str, n_results: int = 3) -> Dict:\n","    \"\"\"\n","    Retrieve the most relevant documents for a query.\n","\n","    Parameters:\n","    -----------\n","    query : str\n","        User's question or search query\n","    n_results : int\n","        Number of top results to return\n","\n","    Returns:\n","    --------\n","    Dictionary with retrieved documents and metadata\n","    \"\"\"\n","    # Generate query embedding\n","    query_embedding = embedding_model.encode([query])[0]\n","\n","    # Search in vector database\n","    results = collection.query(\n","        query_embeddings=[query_embedding.tolist()],\n","        n_results=n_results\n","    )\n","\n","    return results\n","\n","\n","# Test retrieval\n","test_query = \"How do I evaluate a classification model?\"\n","print(f\" Query: {test_query}\\n\")\n","\n","results = retrieve_documents(test_query, n_results=3)\n","\n","print(\"\\nðŸ“„ Retrieved Documents:\\n\")\n","for i, (doc, metadata, distance) in enumerate(zip(results['documents'][0],\n","                                                   results['metadatas'][0],\n","                                                   results['distances'][0]), 1):\n","    print(f\"Result {i} (similarity: {1 - distance:.3f}):\")\n","    print(f\"Title: {metadata['title']}\")\n","    print(f\"Text: {doc[:200]}...\\n\")"]},{"cell_type":"markdown","metadata":{"id":"n9g7A51JPU7T"},"source":["---\n","##  Step 6: Generate Answers with LLM\n","\n","Now we combine retrieval with generation - the heart of RAG!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GsBsyrwLPU7T"},"outputs":[],"source":["def generate_answer(query: str, n_results: int = 3, model: str = \"gpt-3.5-turbo\") -> Dict:\n","    \"\"\"\n","    Generate an answer using RAG: Retrieve relevant docs + Generate with LLM.\n","\n","    Parameters:\n","    -----------\n","    query : str\n","        User's question\n","    n_results : int\n","        Number of documents to retrieve\n","    model : str\n","        OpenAI model to use\n","\n","    Returns:\n","    --------\n","    Dictionary with answer, sources, and metadata\n","    \"\"\"\n","    # Step 1: Retrieve relevant documents\n","    results = retrieve_documents(query, n_results=n_results)\n","\n","    # Step 2: Prepare context from retrieved documents\n","    context = \"\\n\\n\".join([\n","        f\"Document {i+1} ({results['metadatas'][0][i]['title']}):\\n{doc}\"\n","        for i, doc in enumerate(results['documents'][0])\n","    ])\n","\n","    # Step 3: Create prompt with context\n","    system_prompt = \"\"\"You are a helpful data science tutor.\n","    Answer questions based on the provided context documents.\n","    If the context doesn't contain enough information, say so.\n","    Always cite which document(s) you used.\"\"\"\n","\n","    user_prompt = f\"\"\"Context:\n","{context}\n","\n","Question: {query}\n","\n","Please provide a clear, accurate answer based on the context above.\"\"\"\n","\n","    # Step 4: Generate answer with LLM\n","    response = client.chat.completions.create(\n","        model=model,\n","        messages=[\n","            {\"role\": \"system\", \"content\": system_prompt},\n","            {\"role\": \"user\", \"content\": user_prompt}\n","        ],\n","        temperature=0.7,\n","        max_tokens=500\n","    )\n","\n","    answer = response.choices[0].message.content\n","\n","    # Step 5: Package results\n","    return {\n","        'query': query,\n","        'answer': answer,\n","        'sources': [\n","            {\n","                'title': metadata['title'],\n","                'text': doc[:200] + \"...\",\n","                'similarity': 1 - distance\n","            }\n","            for doc, metadata, distance in zip(results['documents'][0],\n","                                               results['metadatas'][0],\n","                                               results['distances'][0])\n","        ]\n","    }\n","\n","\n","# Test the RAG system!\n","test_questions = [\n","    \"How do I evaluate a classification model?\",\n","    \"What is the GIGO principle and why does it matter?\",\n","    \"What's the difference between correlation and causation?\"\n","]\n","\n","for question in test_questions:\n","    print(\"=\" * 80)\n","    print(f\"\\nâ“ Question: {question}\\n\")\n","\n","    result = generate_answer(question)\n","\n","    print(f\"ðŸ’¡ Answer:\\n{result['answer']}\\n\")\n","\n","    print(\" Sources used:\")\n","    for i, source in enumerate(result['sources'], 1):\n","        print(f\"  {i}. {source['title']} (similarity: {source['similarity']:.3f})\")\n","\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"sTht4uU5PU7U"},"source":["---\n","#  Congratulations! You've built a working RAG system!\n","\n","##  What you've accomplished:\n","1.  Created and chunked a document knowledge base\n","2.  Generated semantic embeddings\n","3.  Stored embeddings in a vector database\n","4.  Implemented similarity search retrieval\n","5.  Combined retrieval with LLM generation\n","\n","---\n","\n","##  What's Next?\n","\n","In the following sections, we'll explore:\n","- **Advanced chunking strategies**\n","- **Hybrid search** (keyword + semantic)\n","- **Re-ranking** for better relevance\n","- **Evaluation metrics** for RAG quality\n","- **Production considerations**"]},{"cell_type":"markdown","metadata":{"id":"GTEvyhp8PU7U"},"source":["---\n","# Part 3: Advanced RAG Techniques\n","\n","Now let's level up with advanced techniques that improve RAG performance!"]},{"cell_type":"markdown","metadata":{"id":"rF2Pgf2bPU7U"},"source":["##  Advanced Chunking Strategies\n","\n","### Problem with Simple Chunking:\n","- May split in the middle of important context\n","- Doesn't respect semantic boundaries\n","\n","### Better Approaches:\n","1. **Sentence-aware chunking** - Split at sentence boundaries\n","2. **Sliding window** - Overlapping chunks maintain context\n","3. **Semantic chunking** - Split based on topic changes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9_-xex8PU7U"},"outputs":[],"source":["class AdvancedChunker:\n","    \"\"\"\n","    Advanced chunking with multiple strategies.\n","    \"\"\"\n","\n","    def __init__(self, chunk_size: int = 500, overlap: int = 100, method: str = 'sentence'):\n","        \"\"\"\n","        Parameters:\n","        -----------\n","        chunk_size : int\n","            Target chunk size in characters\n","        overlap : int\n","            Overlap between chunks\n","        method : str\n","            Chunking method: 'fixed', 'sentence', or 'semantic'\n","        \"\"\"\n","        self.chunk_size = chunk_size\n","        self.overlap = overlap\n","        self.method = method\n","\n","    def _split_into_sentences(self, text: str) -> List[str]:\n","        \"\"\"Split text into sentences using regex.\"\"\"\n","        # Simple sentence splitter (can be improved with NLTK or spaCy)\n","        sentences = re.split(r'(?<=[.!?])\\s+', text)\n","        return [s.strip() for s in sentences if s.strip()]\n","\n","    def chunk_by_sentences(self, text: str, metadata: Dict = None) -> List[Dict]:\n","        \"\"\"\n","        Chunk text by grouping sentences up to chunk_size.\n","        Better respects semantic boundaries!\n","        \"\"\"\n","        sentences = self._split_into_sentences(text)\n","        chunks = []\n","        current_chunk = []\n","        current_size = 0\n","\n","        for sentence in sentences:\n","            sentence_len = len(sentence)\n","\n","            # If adding this sentence exceeds chunk_size, save current chunk\n","            if current_size + sentence_len > self.chunk_size and current_chunk:\n","                chunk_text = ' '.join(current_chunk)\n","                chunks.append({\n","                    'text': chunk_text,\n","                    'chunk_id': len(chunks),\n","                    'metadata': metadata or {},\n","                    'num_sentences': len(current_chunk)\n","                })\n","\n","                # Start new chunk with overlap (keep last sentence)\n","                if self.overlap > 0 and current_chunk:\n","                    current_chunk = [current_chunk[-1]]\n","                    current_size = len(current_chunk[0])\n","                else:\n","                    current_chunk = []\n","                    current_size = 0\n","\n","            current_chunk.append(sentence)\n","            current_size += sentence_len\n","\n","        # Add final chunk\n","        if current_chunk:\n","            chunks.append({\n","                'text': ' '.join(current_chunk),\n","                'chunk_id': len(chunks),\n","                'metadata': metadata or {},\n","                'num_sentences': len(current_chunk)\n","            })\n","\n","        return chunks\n","\n","\n","# Compare chunking strategies\n","sample_text = documents[0]['content']\n","\n","# Simple chunking\n","simple_chunker = SimpleChunker(chunk_size=300, overlap=50)\n","simple_chunks = simple_chunker.chunk_text(sample_text)\n","\n","# Advanced sentence-based chunking\n","advanced_chunker = AdvancedChunker(chunk_size=300, overlap=1, method='sentence')\n","advanced_chunks = advanced_chunker.chunk_by_sentences(sample_text)\n","\n","print(\"ðŸ“Š Chunking Comparison:\\n\")\n","print(f\"Simple Chunking:\")\n","print(f\"  â€¢ Number of chunks: {len(simple_chunks)}\")\n","print(f\"  â€¢ Average chunk length: {np.mean([len(c['text']) for c in simple_chunks]):.0f} chars\\n\")\n","\n","print(f\"Sentence-Based Chunking:\")\n","print(f\"  â€¢ Number of chunks: {len(advanced_chunks)}\")\n","print(f\"  â€¢ Average chunk length: {np.mean([len(c['text']) for c in advanced_chunks]):.0f} chars\")\n","print(f\"  â€¢ Average sentences per chunk: {np.mean([c['num_sentences'] for c in advanced_chunks]):.1f}\")\n","\n","print(\"\\nðŸ’¡ Sentence-based chunking respects natural boundaries!\")"]},{"cell_type":"markdown","metadata":{"id":"ObD6O2N6PU7U"},"source":["---\n","##  Hybrid Search: Combining Semantic + Keyword Search\n","\n","**Why hybrid?**\n","- Semantic search: Great for meaning and context\n","- Keyword search (BM25): Better for exact terms and proper nouns\n","- **Hybrid = Best of both worlds!**\n","\n","Example:\n","- Query: \"ARIMA model\" â†’ Keyword search finds exact \"ARIMA\" mentions\n","- Query: \"How to predict future values\" â†’ Semantic search understands intent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WjGryv6VPU7U"},"outputs":[],"source":["from rank_bm25 import BM25Okapi\n","import nltk\n","# Download required NLTK data (run once)\n","# nltk.download('punkt')\n","\n","class HybridRetriever:\n","    \"\"\"\n","    Hybrid retrieval combining semantic (embedding) and keyword (BM25) search.\n","    \"\"\"\n","\n","    def __init__(self, documents: List[str], embeddings: np.ndarray, alpha: float = 0.5):\n","        \"\"\"\n","        Parameters:\n","        -----------\n","        documents : List[str]\n","            List of document texts\n","        embeddings : np.ndarray\n","            Document embeddings\n","        alpha : float\n","            Weight for semantic search (1-alpha for keyword search)\n","            0.5 = equal weight, 0.7 = favor semantic, 0.3 = favor keyword\n","        \"\"\"\n","        self.documents = documents\n","        self.embeddings = embeddings\n","        self.alpha = alpha\n","\n","        # Initialize BM25\n","        tokenized_docs = [doc.lower().split() for doc in documents]\n","        self.bm25 = BM25Okapi(tokenized_docs)\n","\n","    def search(self, query: str, n_results: int = 5) -> List[Tuple[int, float, str]]:\n","        \"\"\"\n","        Perform hybrid search.\n","\n","        Returns:\n","        --------\n","        List of (index, score, source) tuples\n","        \"\"\"\n","        # 1. Semantic search scores\n","        query_embedding = embedding_model.encode([query])[0]\n","        semantic_scores = cosine_similarity([query_embedding], self.embeddings)[0]\n","\n","        # 2. Keyword search scores (BM25)\n","        tokenized_query = query.lower().split()\n","        keyword_scores = self.bm25.get_scores(tokenized_query)\n","\n","        # 3. Normalize scores to [0, 1]\n","        semantic_scores_norm = (semantic_scores - semantic_scores.min()) / (semantic_scores.max() - semantic_scores.min() + 1e-10)\n","        keyword_scores_norm = (keyword_scores - keyword_scores.min()) / (keyword_scores.max() - keyword_scores.min() + 1e-10)\n","\n","        # 4. Combine scores\n","        hybrid_scores = self.alpha * semantic_scores_norm + (1 - self.alpha) * keyword_scores_norm\n","\n","        # 5. Get top results\n","        top_indices = np.argsort(hybrid_scores)[::-1][:n_results]\n","\n","        results = [\n","            (idx, hybrid_scores[idx], f\"semantic: {semantic_scores_norm[idx]:.3f}, keyword: {keyword_scores_norm[idx]:.3f}\")\n","            for idx in top_indices\n","        ]\n","\n","        return results\n","\n","\n","# Test hybrid search\n","hybrid_retriever = HybridRetriever(\n","    documents=chunk_texts,\n","    embeddings=embeddings,\n","    alpha=0.6  # 60% semantic, 40% keyword\n",")\n","\n","# Test queries\n","test_queries = [\n","    \"ARIMA forecasting method\",  # Should benefit from keyword search\n","    \"How to predict future values in time series\"  # Should benefit from semantic search\n","]\n","\n","for query in test_queries:\n","    print(f\"\\nðŸ” Query: {query}\")\n","    print(\"=\"*80)\n","\n","    results = hybrid_retriever.search(query, n_results=3)\n","\n","    for i, (idx, score, breakdown) in enumerate(results, 1):\n","        print(f\"\\nResult {i} (score: {score:.3f}):\")\n","        print(f\"  {breakdown}\")\n","        print(f\"  Title: {all_chunks[idx]['metadata']['title']}\")\n","        print(f\"  Text: {chunk_texts[idx][:150]}...\")"]},{"cell_type":"markdown","metadata":{"id":"hdgjwJs7PU7U"},"source":["---\n","##  Re-Ranking: Improving Retrieval Quality\n","\n","**The Problem:**\n","- Initial retrieval might return relevant but not *optimal* results\n","- Order matters! LLMs work best with most relevant context first\n","\n","**Solution: Re-ranking**\n","1. Retrieve broader set (e.g., top 20)\n","2. Use a specialized re-ranker model\n","3. Return top-K after re-ranking\n","\n","**Popular re-rankers:**\n","- Cross-encoders (more accurate but slower)\n","- LLM-based re-ranking\n","- Custom trained models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"doMhypD7PU7U"},"outputs":[],"source":["from sentence_transformers import CrossEncoder\n","\n","class ReRanker:\n","    \"\"\"\n","    Re-rank retrieved documents using a cross-encoder model.\n","    \"\"\"\n","\n","    def __init__(self, model_name: str = 'cross-encoder/ms-marco-MiniLM-L-6-v2'):\n","        \"\"\"\n","        Initialize re-ranker with a cross-encoder model.\n","        This model directly scores query-document pairs.\n","        \"\"\"\n","        self.model = CrossEncoder(model_name)\n","\n","    def rerank(self, query: str, documents: List[str], top_k: int = 3) -> List[Tuple[int, float]]:\n","        \"\"\"\n","        Re-rank documents for a query.\n","\n","        Returns:\n","        --------\n","        List of (original_index, rerank_score) tuples, sorted by score\n","        \"\"\"\n","        # Score all query-document pairs\n","        pairs = [[query, doc] for doc in documents]\n","        scores = self.model.predict(pairs)\n","\n","        # Sort by score and return top-k\n","        ranked_results = sorted(\n","            enumerate(scores),\n","            key=lambda x: x[1],\n","            reverse=True\n","        )[:top_k]\n","\n","        return ranked_results\n","\n","\n","# Initialize re-ranker\n","reranker = ReRanker()\n","\n","# Compare: Retrieval vs. Retrieval + Re-ranking\n","query = \"What metrics should I use to evaluate my machine learning model?\"\n","\n","print(f\"ðŸ” Query: {query}\\n\")\n","\n","# Step 1: Initial retrieval (get more candidates)\n","initial_results = retrieve_documents(query, n_results=10)\n","candidate_docs = initial_results['documents'][0]\n","\n","print(\" Top 3 WITHOUT Re-ranking:\")\n","for i in range(3):\n","    print(f\"\\n{i+1}. {initial_results['metadatas'][0][i]['title']}\")\n","    print(f\"   Similarity: {1 - initial_results['distances'][0][i]:.3f}\")\n","    print(f\"   {candidate_docs[i][:150]}...\")\n","\n","# Step 2: Re-rank\n","reranked = reranker.rerank(query, candidate_docs, top_k=3)\n","\n","print(\"\\n\\nðŸŽ¯ Top 3 WITH Re-ranking:\")\n","for rank, (orig_idx, score) in enumerate(reranked, 1):\n","    print(f\"\\n{rank}. {initial_results['metadatas'][0][orig_idx]['title']}\")\n","    print(f\"   Re-rank score: {score:.3f}\")\n","    print(f\"   {candidate_docs[orig_idx][:150]}...\")\n","\n","print(\"\\n\\nðŸ’¡ Notice how re-ranking can change the order to be more relevant!\")"]},{"cell_type":"markdown","metadata":{"id":"Tw2yj4h5PU7V"},"source":["---\n","##  Query Enhancement Techniques\n","\n","**Problem:** User queries are often ambiguous or poorly formulated\n","\n","**Solutions:**\n","1. **Query expansion** - Add related terms\n","2. **Query rewriting** - Reformulate for better retrieval\n","3. **Multi-query** - Generate multiple perspectives\n","4. **HyDE** (Hypothetical Document Embeddings) - Generate ideal answer, search with that"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gXgrG0zUPU7V"},"outputs":[],"source":["def expand_query_with_llm(query: str) -> List[str]:\n","    \"\"\"\n","    Generate multiple reformulations of a query using LLM.\n","    This helps capture different aspects and improves recall!\n","    \"\"\"\n","    prompt = f\"\"\"Given this user question, generate 3 different ways to phrase it\n","    that would help retrieve relevant information. Make them diverse.\n","\n","    Original question: {query}\n","\n","    Provide 3 reformulations, one per line, without numbering:\"\"\"\n","\n","    response = client.chat.completions.create(\n","        model=\"gpt-3.5-turbo\",\n","        messages=[{\"role\": \"user\", \"content\": prompt}],\n","        temperature=0.7,\n","        max_tokens=200\n","    )\n","\n","    reformulations = response.choices[0].message.content.strip().split('\\n')\n","    reformulations = [r.strip() for r in reformulations if r.strip()]\n","\n","    return [query] + reformulations[:3]  # Original + 3 variations\n","\n","\n","def multi_query_retrieval(query: str, n_results_per_query: int = 3) -> Dict:\n","    \"\"\"\n","    Retrieve documents using multiple query formulations and merge results.\n","    \"\"\"\n","    # Generate query variations\n","    queries = expand_query_with_llm(query)\n","\n","    print(\"ðŸ”„ Generated query variations:\")\n","    for i, q in enumerate(queries, 1):\n","        print(f\"  {i}. {q}\")\n","\n","    # Retrieve for each query variation\n","    all_results = []\n","    seen_docs = set()\n","\n","    for q in queries:\n","        results = retrieve_documents(q, n_results=n_results_per_query)\n","\n","        for doc, metadata, distance in zip(results['documents'][0],\n","                                           results['metadatas'][0],\n","                                           results['distances'][0]):\n","            doc_id = metadata['title'] + doc[:50]  # Simple deduplication\n","            if doc_id not in seen_docs:\n","                all_results.append({\n","                    'document': doc,\n","                    'metadata': metadata,\n","                    'score': 1 - distance\n","                })\n","                seen_docs.add(doc_id)\n","\n","    # Sort by score and return top results\n","    all_results.sort(key=lambda x: x['score'], reverse=True)\n","\n","    return {\n","        'original_query': query,\n","        'query_variations': queries,\n","        'results': all_results[:n_results_per_query * 2]  # Return more diverse results\n","    }\n","\n","\n","# Test multi-query retrieval\n","test_query = \"How do I check if my data is good?\"\n","\n","print(f\"\\nâ“ Original Query: {test_query}\\n\")\n","print(\"=\"*80)\n","\n","multi_results = multi_query_retrieval(test_query)\n","\n","print(\"\\n\\nðŸ“„ Top Retrieved Documents:\")\n","for i, result in enumerate(multi_results['results'][:3], 1):\n","    print(f\"\\n{i}. {result['metadata']['title']} (score: {result['score']:.3f})\")\n","    print(f\"   {result['document'][:200]}...\")"]},{"cell_type":"markdown","metadata":{"id":"WukO0hohPU7V"},"source":["---\n","# Part 4: RAG Evaluation & Optimization\n","\n","**Critical Question:** How do we know if our RAG system is working well?\n","\n","We need to evaluate both:\n","1. **Retrieval quality** - Are we finding the right documents?\n","2. **Generation quality** - Are the answers accurate and helpful?"]},{"cell_type":"markdown","metadata":{"id":"tc79Vqr1PU7V"},"source":["##  Retrieval Metrics\n","\n","Key metrics for evaluating retrieval:\n","\n","1. **Precision@K** - Of the K retrieved docs, how many are relevant?\n","2. **Recall@K** - Of all relevant docs, how many did we retrieve?\n","3. **MRR (Mean Reciprocal Rank)** - Position of first relevant doc\n","4. **NDCG (Normalized Discounted Cumulative Gain)** - Considers ranking quality"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JpUmXN4RPU7V"},"outputs":[],"source":["# Create evaluation dataset\n","evaluation_questions = [\n","    {\n","        \"question\": \"How do I evaluate a classification model?\",\n","        \"relevant_docs\": [\"Model Evaluation Metrics\"],  # Ground truth\n","        \"category\": \"metrics\"\n","    },\n","    {\n","        \"question\": \"What is GIGO and why does it matter?\",\n","        \"relevant_docs\": [\"GIGO Principle\"],\n","        \"category\": \"data_quality\"\n","    },\n","    {\n","        \"question\": \"How can I predict future sales?\",\n","        \"relevant_docs\": [\"Time Series Forecasting\"],\n","        \"category\": \"forecasting\"\n","    },\n","    {\n","        \"question\": \"What's the difference between correlation and causation?\",\n","        \"relevant_docs\": [\"Causal Inference\"],\n","        \"category\": \"causality\"\n","    },\n","    {\n","        \"question\": \"How do I create better features for my model?\",\n","        \"relevant_docs\": [\"Feature Engineering\"],\n","        \"category\": \"feature_eng\"\n","    }\n","]\n","\n","def calculate_precision_at_k(retrieved_docs: List[str], relevant_docs: List[str], k: int) -> float:\n","    \"\"\"Calculate Precision@K\"\"\"\n","    retrieved_k = retrieved_docs[:k]\n","    relevant_count = sum(1 for doc in retrieved_k if doc in relevant_docs)\n","    return relevant_count / k if k > 0 else 0\n","\n","def calculate_recall_at_k(retrieved_docs: List[str], relevant_docs: List[str], k: int) -> float:\n","    \"\"\"Calculate Recall@K\"\"\"\n","    retrieved_k = retrieved_docs[:k]\n","    relevant_count = sum(1 for doc in retrieved_k if doc in relevant_docs)\n","    return relevant_count / len(relevant_docs) if len(relevant_docs) > 0 else 0\n","\n","def calculate_mrr(retrieved_docs: List[str], relevant_docs: List[str]) -> float:\n","    \"\"\"Calculate Mean Reciprocal Rank\"\"\"\n","    for i, doc in enumerate(retrieved_docs, 1):\n","        if doc in relevant_docs:\n","            return 1 / i\n","    return 0\n","\n","# Evaluate retrieval performance\n","results = []\n","k_values = [1, 3, 5]\n","\n","for eval_item in evaluation_questions:\n","    question = eval_item['question']\n","    relevant_docs = eval_item['relevant_docs']\n","\n","    # Retrieve documents\n","    retrieval_results = retrieve_documents(question, n_results=5)\n","    retrieved_titles = [meta['title'] for meta in retrieval_results['metadatas'][0]]\n","\n","    # Calculate metrics\n","    metrics = {\n","        'question': question,\n","        'category': eval_item['category']\n","    }\n","\n","    for k in k_values:\n","        metrics[f'precision@{k}'] = calculate_precision_at_k(retrieved_titles, relevant_docs, k)\n","        metrics[f'recall@{k}'] = calculate_recall_at_k(retrieved_titles, relevant_docs, k)\n","\n","    metrics['mrr'] = calculate_mrr(retrieved_titles, relevant_docs)\n","    results.append(metrics)\n","\n","# Create DataFrame for analysis\n","eval_df = pd.DataFrame(results)\n","\n","print(\" Retrieval Evaluation Results:\\n\")\n","print(eval_df[['question', 'precision@3', 'recall@3', 'mrr']].to_string(index=False))\n","\n","print(\"\\n\\nðŸ“ˆ Average Metrics:\")\n","for k in k_values:\n","    avg_precision = eval_df[f'precision@{k}'].mean()\n","    avg_recall = eval_df[f'recall@{k}'].mean()\n","    print(f\"  Precision@{k}: {avg_precision:.3f}\")\n","    print(f\"  Recall@{k}: {avg_recall:.3f}\")\n","print(f\"\\n  Mean Reciprocal Rank: {eval_df['mrr'].mean():.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"hc80X2FjPU7i"},"source":["###  Visualize Retrieval Performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJgPXGSHPU7i"},"outputs":[],"source":["# Plot metrics by category\n","fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","# Precision and Recall at different K values\n","metric_cols = [col for col in eval_df.columns if 'precision' in col or 'recall' in col]\n","eval_df[metric_cols].mean().plot(kind='bar', ax=axes[0], color=['skyblue', 'lightcoral', 'lightgreen', 'orange', 'purple', 'pink'])\n","axes[0].set_title('Average Precision and Recall @K', fontweight='bold')\n","axes[0].set_ylabel('Score')\n","axes[0].set_xlabel('Metric')\n","axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n","axes[0].grid(axis='y', alpha=0.3)\n","\n","# MRR by category\n","category_mrr = eval_df.groupby('category')['mrr'].mean().sort_values(ascending=False)\n","category_mrr.plot(kind='barh', ax=axes[1], color='coral')\n","axes[1].set_title('Mean Reciprocal Rank by Category', fontweight='bold')\n","axes[1].set_xlabel('MRR Score')\n","axes[1].set_ylabel('Category')\n","axes[1].grid(axis='x', alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"arNO-NIVPU7i"},"source":["---\n","##  End-to-End RAG Evaluation\n","\n","Now let's evaluate the complete RAG system (retrieval + generation):\n","\n","**Key metrics:**\n","1. **Faithfulness** - Is the answer grounded in retrieved context?\n","2. **Answer Relevance** - Does it address the question?\n","3. **Context Relevance** - Are retrieved docs relevant?\n","4. **Correctness** - Is the answer factually correct?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-v3oT-7PU7i"},"outputs":[],"source":["def evaluate_rag_answer(question: str, answer: str, context: str) -> Dict:\n","    \"\"\"\n","    Evaluate a RAG answer using LLM-as-judge approach.\n","    \"\"\"\n","    evaluation_prompt = f\"\"\"Evaluate this RAG system answer on a scale of 1-5 for each criterion:\n","\n","Question: {question}\n","\n","Retrieved Context:\n","{context}\n","\n","Generated Answer:\n","{answer}\n","\n","Rate the following (1=Poor, 5=Excellent):\n","1. Faithfulness: Is the answer based on the provided context (not hallucinated)?\n","2. Relevance: Does the answer directly address the question?\n","3. Completeness: Is the answer comprehensive enough?\n","4. Clarity: Is the answer clear and well-explained?\n","\n","Provide ratings in this exact format:\n","Faithfulness: X\n","Relevance: X\n","Completeness: X\n","Clarity: X\n","Overall: X\n","Reasoning: [brief explanation]\"\"\"\n","\n","    response = client.chat.completions.create(\n","        model=\"gpt-3.5-turbo\",\n","        messages=[{\"role\": \"user\", \"content\": evaluation_prompt}],\n","        temperature=0.3\n","    )\n","\n","    eval_text = response.choices[0].message.content\n","\n","    # Parse scores\n","    scores = {}\n","    for line in eval_text.split('\\n'):\n","        if ':' in line:\n","            key, value = line.split(':', 1)\n","            key = key.strip().lower()\n","            if key in ['faithfulness', 'relevance', 'completeness', 'clarity', 'overall']:\n","                try:\n","                    scores[key] = int(value.strip().split()[0])\n","                except:\n","                    pass\n","\n","    return scores\n","\n","\n","# Evaluate RAG system end-to-end\n","print(\"ðŸ” Evaluating RAG System End-to-End...\\n\")\n","\n","rag_evaluations = []\n","\n","for eval_item in evaluation_questions[:3]:  # Test first 3\n","    question = eval_item['question']\n","\n","    # Generate answer\n","    result = generate_answer(question, n_results=3)\n","\n","    # Prepare context\n","    context = \"\\n\\n\".join([source['text'] for source in result['sources']])\n","\n","    # Evaluate\n","    scores = evaluate_rag_answer(question, result['answer'], context)\n","\n","    rag_evaluations.append({\n","        'question': question,\n","        **scores\n","    })\n","\n","    print(f\"Question: {question}\")\n","    print(f\"Scores: {scores}\\n\")\n","\n","# Summary\n","eval_df = pd.DataFrame(rag_evaluations)\n","print(\"\\nðŸ“Š Average Scores:\")\n","for col in ['faithfulness', 'relevance', 'completeness', 'clarity', 'overall']:\n","    if col in eval_df.columns:\n","        print(f\"  {col.capitalize()}: {eval_df[col].mean():.2f}/5\")"]},{"cell_type":"markdown","metadata":{"id":"pZjfZDnzPU7j"},"source":["---\n","# Part 5: Production Considerations & Best Practices\n","\n","Moving from prototype to production RAG system"]},{"cell_type":"markdown","metadata":{"id":"3G5vM5IqPU7j"},"source":["##  Optimization Strategies\n","\n","### 1. **Caching**\n","- Cache embeddings (don't recompute for same texts)\n","- Cache LLM responses for common queries\n","\n","### 2. **Batch Processing**\n","- Generate embeddings in batches\n","- Use async calls for LLM generation\n","\n","### 3. **Index Optimization**\n","- Use approximate nearest neighbor (ANN) algorithms\n","- Consider HNSW, IVF indexes for large scale\n","\n","### 4. **Cost Optimization**\n","- Use smaller embedding models when possible\n","- Implement retrieval thresholds\n","- Monitor token usage"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FLOkwEJAPU7j"},"outputs":[],"source":["class ProductionRAGSystem:\n","    \"\"\"\n","    Production-ready RAG system with optimization and monitoring.\n","    \"\"\"\n","\n","    def __init__(self, collection_name: str, embedding_model_name: str = 'all-MiniLM-L6-v2'):\n","        self.collection_name = collection_name\n","        self.embedding_model = SentenceTransformer(embedding_model_name)\n","        self.chroma_client = chromadb.Client()\n","\n","        # Metrics tracking\n","        self.metrics = {\n","            'queries': 0,\n","            'total_latency': 0,\n","            'retrieval_latency': 0,\n","            'generation_latency': 0,\n","            'errors': 0\n","        }\n","\n","        # Simple cache\n","        self.response_cache = {}\n","        self.cache_hits = 0\n","\n","    def query(self, question: str, use_cache: bool = True, n_results: int = 3) -> Dict:\n","        \"\"\"\n","        Query the RAG system with caching and monitoring.\n","        \"\"\"\n","        import time\n","        start_time = time.time()\n","\n","        try:\n","            # Check cache\n","            if use_cache and question in self.response_cache:\n","                self.cache_hits += 1\n","                return self.response_cache[question]\n","\n","            # Retrieval\n","            retrieval_start = time.time()\n","            results = retrieve_documents(question, n_results=n_results)\n","            retrieval_time = time.time() - retrieval_start\n","\n","            # Generation\n","            generation_start = time.time()\n","            answer = generate_answer(question, n_results=n_results)\n","            generation_time = time.time() - generation_start\n","\n","            # Update metrics\n","            total_time = time.time() - start_time\n","            self.metrics['queries'] += 1\n","            self.metrics['total_latency'] += total_time\n","            self.metrics['retrieval_latency'] += retrieval_time\n","            self.metrics['generation_latency'] += generation_time\n","\n","            # Cache result\n","            if use_cache:\n","                self.response_cache[question] = answer\n","\n","            # Add timing info\n","            answer['timing'] = {\n","                'total': total_time,\n","                'retrieval': retrieval_time,\n","                'generation': generation_time\n","            }\n","\n","            return answer\n","\n","        except Exception as e:\n","            self.metrics['errors'] += 1\n","            return {'error': str(e)}\n","\n","    def get_metrics(self) -> Dict:\n","        \"\"\"Get performance metrics.\"\"\"\n","        n_queries = max(self.metrics['queries'], 1)\n","        return {\n","            'total_queries': self.metrics['queries'],\n","            'cache_hit_rate': self.cache_hits / n_queries if n_queries > 0 else 0,\n","            'avg_total_latency': self.metrics['total_latency'] / n_queries,\n","            'avg_retrieval_latency': self.metrics['retrieval_latency'] / n_queries,\n","            'avg_generation_latency': self.metrics['generation_latency'] / n_queries,\n","            'error_rate': self.metrics['errors'] / n_queries if n_queries > 0 else 0\n","        }\n","\n","\n","# Test production system\n","prod_rag = ProductionRAGSystem(collection_name=\"data_science_docs\")\n","\n","# Run queries\n","test_queries = [\n","    \"What is the GIGO principle?\",\n","    \"How do I evaluate my model?\",\n","    \"What is the GIGO principle?\",  # Duplicate - should hit cache\n","]\n","\n","print(\" Testing Production RAG System...\\n\")\n","\n","for query in test_queries:\n","    result = prod_rag.query(query)\n","    print(f\"Query: {query}\")\n","    print(f\"  Latency: {result.get('timing', {}).get('total', 0):.3f}s\")\n","    print()\n","\n","# Print metrics\n","print(\"\\n System Metrics:\")\n","metrics = prod_rag.get_metrics()\n","for key, value in metrics.items():\n","    if 'latency' in key:\n","        print(f\"  {key}: {value:.3f}s\")\n","    elif 'rate' in key:\n","        print(f\"  {key}: {value:.1%}\")\n","    else:\n","        print(f\"  {key}: {value}\")"]},{"cell_type":"markdown","metadata":{"id":"VqcrAwQAPU7j"},"source":["---\n","##  Common Pitfalls & Solutions\n","\n","### Problem 1: **Context Length Exceeded**\n","**Symptoms:** Error when retrieved context + query > model's context window\n","\n","**Solutions:**\n","- Reduce number of retrieved documents\n","- Truncate documents intelligently\n","- Use models with larger context windows\n","\n","### Problem 2: **Poor Retrieval Quality**\n","**Symptoms:** Irrelevant documents retrieved\n","\n","**Solutions:**\n","- Improve chunking strategy\n","- Try different embedding models\n","- Implement hybrid search\n","- Add metadata filtering\n","\n","### Problem 3: **Hallucinations**\n","**Symptoms:** LLM generates information not in context\n","\n","**Solutions:**\n","- Strengthen system prompt (\"only use provided context\")\n","- Lower temperature\n","- Implement fact-checking\n","- Add citations/sources\n","\n","### Problem 4: **Slow Response Times**\n","**Symptoms:** High latency\n","\n","**Solutions:**\n","- Implement caching\n","- Use faster embedding models\n","- Optimize vector database indexes\n","- Consider async processing"]},{"cell_type":"markdown","metadata":{"id":"Fl4s8mORPU7j"},"source":["---\n","#  Exercises & Practice\n","\n","## Exercise 1: Improve Chunking (Beginner)\n","**Task:** Modify the chunking strategy to preserve paragraph boundaries\n","\n","**Hints:**\n","- Look for double newlines (`\\n\\n`)\n","- Try to keep complete paragraphs together\n","- Compare performance with original chunking"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wuyjbxBPU7j"},"outputs":[],"source":["# YOUR CODE HERE\n","# Implement paragraph-aware chunking\n","\n","def paragraph_aware_chunking(text: str, max_chunk_size: int = 500) -> List[str]:\n","    \"\"\"\n","    TODO: Implement chunking that respects paragraph boundaries.\n","    \"\"\"\n","    pass\n","\n","# Test your implementation"]},{"cell_type":"markdown","metadata":{"id":"pUV_Hs8vPU7j"},"source":["## Exercise 2: Add Metadata Filtering (Intermediate)\n","**Task:** Extend the retrieval system to filter by metadata (e.g., document category, date)\n","\n","**Requirements:**\n","- Add category metadata to documents\n","- Implement filtered retrieval\n","- Test with queries like \"Find forecasting methods\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FK7OHOKHPU7j"},"outputs":[],"source":["# YOUR CODE HERE\n","# Implement metadata filtering\n","\n","def retrieve_with_filter(query: str, category: str = None, n_results: int = 3):\n","    \"\"\"\n","    TODO: Add metadata filtering to retrieval.\n","    \"\"\"\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"PE3YXkvoPU7j"},"source":["## Exercise 3: Build a Conversational RAG (Advanced)\n","**Task:** Extend the RAG system to handle multi-turn conversations\n","\n","**Requirements:**\n","- Maintain conversation history\n","- Use history to contextualize queries\n","- Handle follow-up questions (\"Tell me more about that\")\n","\n","**Bonus:** Implement conversation summarization to stay within context limits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C4OmO1kGPU7j"},"outputs":[],"source":["# YOUR CODE HERE\n","# Implement conversational RAG\n","\n","class ConversationalRAG:\n","    \"\"\"\n","    TODO: Implement RAG system with conversation memory.\n","    \"\"\"\n","    def __init__(self):\n","        self.conversation_history = []\n","\n","    def chat(self, user_input: str) -> str:\n","        pass"]},{"cell_type":"markdown","metadata":{"id":"uGEHR5zOPU7j"},"source":["---\n","#  Project Ideas for Extension\n","\n","## 1. **Domain-Specific RAG**\n","Build a RAG system for a specific domain:\n","- Medical literature assistant\n","- Legal document analyzer\n","- Code documentation helper\n","\n","## 2. **Multi-Modal RAG**\n","Extend to handle images, tables, and structured data:\n","- Extract text from PDFs\n","- Index tables and figures\n","- Image caption generation\n","\n","## 3. **RAG-Powered Chatbot**\n","Build a production chatbot with:\n","- Web interface (Streamlit/Gradio)\n","- User feedback collection\n","- Continuous learning\n","\n","## 4. **Evaluation Suite**\n","Comprehensive evaluation framework:\n","- Automated test dataset generation\n","- Multiple evaluation metrics\n","- A/B testing different RAG configurations\n","\n","## 5. **RAG + Fine-tuning**\n","Combine approaches:\n","- Use RAG for knowledge\n","- Fine-tune for domain language/style\n","- Compare hybrid vs. pure approaches"]},{"cell_type":"markdown","metadata":{"id":"qHbEEuy7PU7j"},"source":["---\n","#  Additional Resources\n","\n","## Papers & Blogs\n","- **RAG Paper:** \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" (Lewis et al., 2020)\n","- **LangChain RAG Guide:** https://python.langchain.com/docs/use_cases/question_answering/\n","- **Pinecone Learning Center:** https://www.pinecone.io/learn/\n","\n","## Tools & Libraries\n","- **LangChain:** RAG orchestration framework\n","- **LlamaIndex:** Data framework for LLM applications\n","- **ChromaDB:** Vector database\n","- **Weaviate:** Production vector database\n","- **RAGAS:** RAG evaluation framework\n","\n","## Advanced Topics\n","- **RAGAs Framework:** Automated RAG evaluation\n","- **LLM Routing:** Directing queries to best source\n","- **Agentic RAG:** Using agents for complex retrieval\n","- **GraphRAG:** Using knowledge graphs with RAG"]},{"cell_type":"markdown","metadata":{"id":"ltqcaIjSPU7k"},"source":["---\n","#  Conclusion\n","\n","## What You've Learned\n","\n"," **Foundations:** Understanding RAG architecture and components  \n"," **Implementation:** Building a working RAG system from scratch  \n"," **Advanced Techniques:** Hybrid search, re-ranking, query enhancement  \n"," **Evaluation:** Measuring and improving RAG performance  \n"," **Production:** Optimization and best practices  \n","\n","## Key Takeaways\n","\n","1. **RAG is powerful but not magic** - Quality depends on your data and configuration\n","2. **Chunking matters** - It's often the most impactful design decision\n","3. **Always evaluate** - Use metrics to guide improvements\n","4. **Start simple** - Add complexity only when needed\n","5. **Monitor in production** - Track performance and user satisfaction\n","\n","## Next Steps\n","\n","1. Complete the exercises in this notebook\n","2. Try the project ideas with your own data\n","3. Experiment with different configurations\n","4. Build something and share it!\n","\n","---\n","\n","**Remember:** The best way to learn is by doing. Start building!\n","\n","**Questions?** Feel free to reach out or check the resources section.\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}